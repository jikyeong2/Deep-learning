# -*- coding: utf-8 -*-
"""정시설_딥러닝(epoch=10)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14nMsqhwCiVZ81jpQPzSjjzRqfIaWxXSK
"""

import os, glob
import scipy
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as img
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import ModelCheckpoint

from google.colab import drive
drive.mount('/content/drive')

tf.keras.preprocessing.image.load_img('/content/drive/MyDrive/정시설 sql/carpark_data/Free/img_1006075057.jpg')

src = '/content/drive/MyDrive/정시설 sql/carpark_data'

## Full
full_list = glob.glob(os.path.join(src, 'Full/*.jpg'))

## Free
free_list = glob.glob(os.path.join(src, 'Free/*.jpg'))

print("Full :", len(full_list))
print("Free :", len(free_list))

## plot
plt.figure(figsize=(16, 16))

plt.subplot(1, 2, 1)
plt.title("Full")
full_image = img.imread(full_list[0])
plt.imshow(full_image)

plt.subplot(1, 2, 2)
plt.title("Free")
free_image = img.imread(free_list[0])
plt.imshow(free_image)

## DataGenerator를 통해 데이터 load 및 argumentation

train_datagen = ImageDataGenerator(
    rotation_range = 10,              ## 회전 범위
    width_shift_range = 0.1,
    height_shift_range = 0.1,
    brightness_range = [0.8, 1.2],    ## 밝기 조절
    shear_range = 0.01,               ## 층밀리기 강도
    zoom_range = [0.9, 1.1],          ## 줌 범위
    validation_split = 0.1,
    preprocessing_function = preprocess_input
)

val_datagen = ImageDataGenerator(
    validation_split = 0.1,
    preprocessing_function = preprocess_input
)

train_gen = train_datagen.flow_from_directory(
    src,
    target_size = (224, 224),
    classes = ['Full', 'Free'],
    class_mode = 'categorical',
    batch_size = 32,
    shuffle = True,
    subset = 'training'
)

val_gen = val_datagen.flow_from_directory(
    src,
    target_size = (224, 224),
    classes = ['Full', 'Free'],
    class_mode = 'categorical',
    batch_size = 32,
    shuffle = False,
    subset = 'validation'
)

print("Train Dataset's Class")
print(val_gen.class_indices)

print("\nValidation Dataset's Class")
print(val_gen.class_indices)

base_model = MobileNetV2(input_shape=(224, 224, 3), weights='imagenet', include_top=False)
base_model.summary()

x = base_model.output
x = GlobalAveragePooling2D()(x)
output = Dense(2, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
model.summary()

for layer in model.layers:
    layer.trainable = True

history = model.fit_generator(
    train_gen,
    validation_data = val_gen,
    epochs = 10,
    callbacks = [
        ModelCheckpoint('/content/drive/MyDrive/정시설 sql', monitor = 'val_loss', save_best_only = True, verbose = 1)
    ]
)

model = load_model('/content/drive/MyDrive/정시설 sql')

test_img_free = img_to_array(load_img(os.path.join(src, 'Free/img_129173058.jpg'), target_size=(224, 224)))
test_input_free = preprocess_input(np.expand_dims(test_img_free.copy(), axis=0))
pred = model.predict(test_input_free)

plt.figure(figsize=(8, 8))
plt.title('%.2f%% Free' % (pred[0][1] * 100))
plt.imshow(test_img_free.astype(np.uint8))

test_img_full = img_to_array(load_img(os.path.join(src, 'Full/img_127061058.jpg'), target_size=(224, 224)))
test_input_full = preprocess_input(np.expand_dims(test_img_full.copy(), axis=0))
pred = model.predict(test_input_full)

plt.figure(figsize=(8, 8))
plt.title('%.2f%% Full' % (pred[0][0] * 100))
plt.imshow(test_img_full.astype(np.uint8))

# 모델의 마지막 레이어의 가중치를 불러옵니다. -> shape : (1280, 2)
last_weight = model.layers[-1].get_weights()[0]

new_model = Model(
    inputs = model.input,
    outputs = (
        model.layers[-3].output,
        model.layers[-1].output
    )
)

new_model.summary()

last_conv_output, pred = new_model.predict(test_input_free)

last_conv_output = np.squeeze(last_conv_output) ## (7, 7, 1280)
feature_activation_maps = scipy.ndimage.zoom(last_conv_output, (32, 32, 1), order=1) ## (7, 7, 1280) -> (224, 224, 1280)

pred_class = np.argmax(pred) ## 0: Full, 1: Free
predicted_class_weights = last_weight[:, pred_class] ## (1280, 1)

## (224*224, 1280) dot_produt (1280, 1) = (224*224, 1)
final_output = np.dot(feature_activation_maps.reshape((224*224, 1280)), predicted_class_weights).reshape((224, 224))
plt.imshow(final_output, cmap='jet')

fig, ax = plt.subplots(nrows=1, ncols=2)
fig.set_size_inches(16, 20)

ax[0].imshow(test_img_free.astype(np.uint8))
ax[0].set_title('image')
ax[0].axis('off')

ax[1].imshow(test_img_free.astype(np.uint8), alpha=0.5)
ax[1].imshow(final_output, cmap='jet', alpha=0.5)
ax[1].set_title('class activation map')
ax[1].axis('off')
plt.show()

last_conv_output, pred = new_model.predict(test_input_full)

last_conv_output = np.squeeze(last_conv_output)
feature_activation_maps = scipy.ndimage.zoom(last_conv_output, (32, 32, 1), order=1)

pred_class = np.argmax(pred)
predicted_class_weights = last_weight[:, pred_class]

final_output = np.dot(feature_activation_maps.reshape((224*224, 1280)), predicted_class_weights).reshape((224, 224))
plt.imshow(final_output, cmap='jet')

fig, ax = plt.subplots(nrows=1, ncols=2)
fig.set_size_inches(16, 20)

ax[0].imshow(test_img_full.astype(np.uint8))
ax[0].set_title('image')
ax[0].axis('off')

ax[1].imshow(test_img_full.astype(np.uint8), alpha=0.5)
ax[1].imshow(final_output, cmap='jet', alpha=0.5)
ax[1].set_title('class activation map')
ax[1].axis('off')
plt.show()